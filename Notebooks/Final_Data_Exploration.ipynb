{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import joblib\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "data=pd.read_csv('../data_store/final_data/final_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>aqi</th>\n",
       "      <th>state</th>\n",
       "      <th>co</th>\n",
       "      <th>no</th>\n",
       "      <th>no2</th>\n",
       "      <th>o3</th>\n",
       "      <th>so2</th>\n",
       "      <th>pm2_5</th>\n",
       "      <th>pm10</th>\n",
       "      <th>nh3</th>\n",
       "      <th>temperature_2m</th>\n",
       "      <th>relative_humidity_2m</th>\n",
       "      <th>rain</th>\n",
       "      <th>wind_speed_10m</th>\n",
       "      <th>wind_direction_10m</th>\n",
       "      <th>soil_temperature_0_to_7cm</th>\n",
       "      <th>soil_moisture_0_to_7cm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-10-13 07:00:00</td>\n",
       "      <td>32.83</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>198.60</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.05</td>\n",
       "      <td>56.51</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2.84</td>\n",
       "      <td>3.31</td>\n",
       "      <td>0.19</td>\n",
       "      <td>16.929998</td>\n",
       "      <td>84.940186</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.594208</td>\n",
       "      <td>58.570484</td>\n",
       "      <td>18.279999</td>\n",
       "      <td>0.445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-10-13 08:00:00</td>\n",
       "      <td>31.95</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>193.60</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.79</td>\n",
       "      <td>55.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>2.74</td>\n",
       "      <td>3.21</td>\n",
       "      <td>0.14</td>\n",
       "      <td>17.230000</td>\n",
       "      <td>84.153740</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.704336</td>\n",
       "      <td>52.594578</td>\n",
       "      <td>18.230000</td>\n",
       "      <td>0.445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-10-13 09:00:00</td>\n",
       "      <td>31.57</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>191.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.67</td>\n",
       "      <td>53.64</td>\n",
       "      <td>0.09</td>\n",
       "      <td>2.81</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.11</td>\n",
       "      <td>17.529999</td>\n",
       "      <td>82.841130</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.287822</td>\n",
       "      <td>55.619600</td>\n",
       "      <td>18.279999</td>\n",
       "      <td>0.444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-10-13 10:00:00</td>\n",
       "      <td>31.25</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>190.26</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.70</td>\n",
       "      <td>52.21</td>\n",
       "      <td>0.13</td>\n",
       "      <td>3.05</td>\n",
       "      <td>3.58</td>\n",
       "      <td>0.09</td>\n",
       "      <td>17.429998</td>\n",
       "      <td>83.905220</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.396570</td>\n",
       "      <td>59.036320</td>\n",
       "      <td>18.279999</td>\n",
       "      <td>0.443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-10-13 11:00:00</td>\n",
       "      <td>31.38</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>191.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.79</td>\n",
       "      <td>50.78</td>\n",
       "      <td>0.17</td>\n",
       "      <td>3.35</td>\n",
       "      <td>3.92</td>\n",
       "      <td>0.08</td>\n",
       "      <td>17.380000</td>\n",
       "      <td>84.442380</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.787991</td>\n",
       "      <td>56.309900</td>\n",
       "      <td>18.230000</td>\n",
       "      <td>0.443</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             timestamp    aqi    state      co   no   no2     o3   so2  pm2_5  \\\n",
       "0  2023-10-13 07:00:00  32.83  Alabama  198.60  0.0  1.05  56.51  0.13   2.84   \n",
       "1  2023-10-13 08:00:00  31.95  Alabama  193.60  0.0  0.79  55.07  0.07   2.74   \n",
       "2  2023-10-13 09:00:00  31.57  Alabama  191.93  0.0  0.67  53.64  0.09   2.81   \n",
       "3  2023-10-13 10:00:00  31.25  Alabama  190.26  0.0  0.70  52.21  0.13   3.05   \n",
       "4  2023-10-13 11:00:00  31.38  Alabama  191.93  0.0  0.79  50.78  0.17   3.35   \n",
       "\n",
       "   pm10   nh3  temperature_2m  relative_humidity_2m  rain  wind_speed_10m  \\\n",
       "0  3.31  0.19       16.929998             84.940186   0.0        7.594208   \n",
       "1  3.21  0.14       17.230000             84.153740   0.0        7.704336   \n",
       "2  3.30  0.11       17.529999             82.841130   0.0        8.287822   \n",
       "3  3.58  0.09       17.429998             83.905220   0.0        8.396570   \n",
       "4  3.92  0.08       17.380000             84.442380   0.0        7.787991   \n",
       "\n",
       "   wind_direction_10m  soil_temperature_0_to_7cm  soil_moisture_0_to_7cm  \n",
       "0           58.570484                  18.279999                   0.445  \n",
       "1           52.594578                  18.230000                   0.445  \n",
       "2           55.619600                  18.279999                   0.444  \n",
       "3           59.036320                  18.279999                   0.443  \n",
       "4           56.309900                  18.230000                   0.443  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model for Alabama\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 245\u001b[0m\n\u001b[1;32m    243\u001b[0m sequence_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m24\u001b[39m\n\u001b[1;32m    244\u001b[0m model \u001b[38;5;241m=\u001b[39m HybridAQIPredictor(sequence_length\u001b[38;5;241m=\u001b[39msequence_length)\n\u001b[0;32m--> 245\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;66;03m# Save the model\u001b[39;00m\n\u001b[1;32m    248\u001b[0m model\u001b[38;5;241m.\u001b[39msave_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhybrid_aqi_model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[31], line 188\u001b[0m, in \u001b[0;36mHybridAQIPredictor.train_and_evaluate\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    183\u001b[0m (X_train_scaled, X_test_scaled,\n\u001b[1;32m    184\u001b[0m  y_train_scaled, y_test_scaled,\n\u001b[1;32m    185\u001b[0m  y_train, y_test) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_data(data, state)\n\u001b[1;32m    187\u001b[0m \u001b[38;5;66;03m# SARIMAX training\u001b[39;00m\n\u001b[0;32m--> 188\u001b[0m sarimax_preds_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_sarimax\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m residuals \u001b[38;5;241m=\u001b[39m y_train \u001b[38;5;241m-\u001b[39m sarimax_preds_train\n\u001b[1;32m    190\u001b[0m residuals_scaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler_y\u001b[38;5;241m.\u001b[39mtransform(residuals\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n",
      "Cell \u001b[0;32mIn[31], line 110\u001b[0m, in \u001b[0;36mHybridAQIPredictor.train_sarimax\u001b[0;34m(self, y_train, exog_train)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_sarimax\u001b[39m(\u001b[38;5;28mself\u001b[39m, y_train, exog_train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;66;03m# Simplified SARIMAX for hourly data\u001b[39;00m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msarimax_model \u001b[38;5;241m=\u001b[39m SARIMAX(y_train,\n\u001b[1;32m    107\u001b[0m                                exog\u001b[38;5;241m=\u001b[39mexog_train,\n\u001b[1;32m    108\u001b[0m                                order\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m    109\u001b[0m                                seasonal_order\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m24\u001b[39m))\n\u001b[0;32m--> 110\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msarimax_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msarimax_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdisp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msarimax_model\u001b[38;5;241m.\u001b[39mpredict()\n",
      "File \u001b[0;32m~/miniconda3/envs/PT_GPU/lib/python3.12/site-packages/statsmodels/tsa/statespace/mlemodel.py:705\u001b[0m, in \u001b[0;36mMLEModel.fit\u001b[0;34m(self, start_params, transformed, includes_fixed, cov_type, cov_kwds, method, maxiter, full_output, disp, callback, return_params, optim_score, optim_complex_step, optim_hessian, flags, low_memory, **kwargs)\u001b[0m\n\u001b[1;32m    703\u001b[0m         flags[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhessian_method\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m optim_hessian\n\u001b[1;32m    704\u001b[0m     fargs \u001b[38;5;241m=\u001b[39m (flags,)\n\u001b[0;32m--> 705\u001b[0m     mlefit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mfargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaxiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mfull_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfull_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mdisp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    710\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mskip_hessian\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    712\u001b[0m \u001b[38;5;66;03m# Just return the fitted parameters if requested\u001b[39;00m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_params:\n",
      "File \u001b[0;32m~/miniconda3/envs/PT_GPU/lib/python3.12/site-packages/statsmodels/base/model.py:566\u001b[0m, in \u001b[0;36mLikelihoodModel.fit\u001b[0;34m(self, start_params, method, maxiter, full_output, disp, fargs, callback, retall, skip_hessian, **kwargs)\u001b[0m\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_t\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    565\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m Optimizer()\n\u001b[0;32m--> 566\u001b[0m xopt, retvals, optim_settings \u001b[38;5;241m=\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mfargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mhessian\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mdisp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaxiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mretall\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretall\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mfull_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfull_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;66;03m# Restore cov_type, cov_kwds and use_t\u001b[39;00m\n\u001b[1;32m    576\u001b[0m optim_settings\u001b[38;5;241m.\u001b[39mupdate(kwds)\n",
      "File \u001b[0;32m~/miniconda3/envs/PT_GPU/lib/python3.12/site-packages/statsmodels/base/optimizer.py:243\u001b[0m, in \u001b[0;36mOptimizer._fit\u001b[0;34m(self, objective, gradient, start_params, fargs, kwargs, hessian, method, maxiter, full_output, disp, callback, retall)\u001b[0m\n\u001b[1;32m    240\u001b[0m     fit_funcs\u001b[38;5;241m.\u001b[39mupdate(extra_fit_funcs)\n\u001b[1;32m    242\u001b[0m func \u001b[38;5;241m=\u001b[39m fit_funcs[method]\n\u001b[0;32m--> 243\u001b[0m xopt, retvals \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mdisp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaxiter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mretall\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfull_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mhess\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhessian\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m optim_settings \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer\u001b[39m\u001b[38;5;124m'\u001b[39m: method, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart_params\u001b[39m\u001b[38;5;124m'\u001b[39m: start_params,\n\u001b[1;32m    249\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaxiter\u001b[39m\u001b[38;5;124m'\u001b[39m: maxiter, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfull_output\u001b[39m\u001b[38;5;124m'\u001b[39m: full_output,\n\u001b[1;32m    250\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisp\u001b[39m\u001b[38;5;124m'\u001b[39m: disp, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfargs\u001b[39m\u001b[38;5;124m'\u001b[39m: fargs, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcallback\u001b[39m\u001b[38;5;124m'\u001b[39m: callback,\n\u001b[1;32m    251\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mretall\u001b[39m\u001b[38;5;124m'\u001b[39m: retall, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextra_fit_funcs\u001b[39m\u001b[38;5;124m\"\u001b[39m: extra_fit_funcs}\n\u001b[1;32m    252\u001b[0m optim_settings\u001b[38;5;241m.\u001b[39mupdate(kwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/PT_GPU/lib/python3.12/site-packages/statsmodels/base/optimizer.py:660\u001b[0m, in \u001b[0;36m_fit_lbfgs\u001b[0;34m(f, score, start_params, fargs, kwargs, disp, maxiter, callback, retall, full_output, hess)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m approx_grad:\n\u001b[1;32m    658\u001b[0m     func \u001b[38;5;241m=\u001b[39m f\n\u001b[0;32m--> 660\u001b[0m retvals \u001b[38;5;241m=\u001b[39m \u001b[43moptimize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfmin_l_bfgs_b\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaxiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    661\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mbounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[43m                                 \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m full_output:\n\u001b[1;32m    666\u001b[0m     xopt, fopt, d \u001b[38;5;241m=\u001b[39m retvals\n",
      "File \u001b[0;32m~/miniconda3/envs/PT_GPU/lib/python3.12/site-packages/scipy/optimize/_lbfgsb_py.py:237\u001b[0m, in \u001b[0;36mfmin_l_bfgs_b\u001b[0;34m(func, x0, fprime, args, approx_grad, bounds, m, factr, pgtol, epsilon, iprint, maxfun, maxiter, disp, callback, maxls)\u001b[0m\n\u001b[1;32m    225\u001b[0m callback \u001b[38;5;241m=\u001b[39m _wrap_callback(callback)\n\u001b[1;32m    226\u001b[0m opts \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisp\u001b[39m\u001b[38;5;124m'\u001b[39m: disp,\n\u001b[1;32m    227\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124miprint\u001b[39m\u001b[38;5;124m'\u001b[39m: iprint,\n\u001b[1;32m    228\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaxcor\u001b[39m\u001b[38;5;124m'\u001b[39m: m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcallback\u001b[39m\u001b[38;5;124m'\u001b[39m: callback,\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaxls\u001b[39m\u001b[38;5;124m'\u001b[39m: maxls}\n\u001b[0;32m--> 237\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43m_minimize_lbfgsb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m                       \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mopts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m d \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrad\u001b[39m\u001b[38;5;124m'\u001b[39m: res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjac\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    240\u001b[0m      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtask\u001b[39m\u001b[38;5;124m'\u001b[39m: res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    241\u001b[0m      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfuncalls\u001b[39m\u001b[38;5;124m'\u001b[39m: res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnfev\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    242\u001b[0m      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnit\u001b[39m\u001b[38;5;124m'\u001b[39m: res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnit\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    243\u001b[0m      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwarnflag\u001b[39m\u001b[38;5;124m'\u001b[39m: res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m'\u001b[39m]}\n\u001b[1;32m    244\u001b[0m f \u001b[38;5;241m=\u001b[39m res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfun\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/PT_GPU/lib/python3.12/site-packages/scipy/optimize/_lbfgsb_py.py:407\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m    401\u001b[0m task_str \u001b[38;5;241m=\u001b[39m task\u001b[38;5;241m.\u001b[39mtobytes()\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m task_str\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFG\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    403\u001b[0m     \u001b[38;5;66;03m# The minimization routine wants f and g at the current x.\u001b[39;00m\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;66;03m# Note that interruptions due to maxfun are postponed\u001b[39;00m\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;66;03m# until the completion of the current minimization iteration.\u001b[39;00m\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;66;03m# Overwrite f and g:\u001b[39;00m\n\u001b[0;32m--> 407\u001b[0m     f, g \u001b[38;5;241m=\u001b[39m \u001b[43mfunc_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m task_str\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNEW_X\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    409\u001b[0m     \u001b[38;5;66;03m# new iteration\u001b[39;00m\n\u001b[1;32m    410\u001b[0m     n_iterations \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/PT_GPU/lib/python3.12/site-packages/scipy/optimize/_differentiable_functions.py:296\u001b[0m, in \u001b[0;36mScalarFunction.fun_and_grad\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray_equal(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx):\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_x_impl(x)\n\u001b[0;32m--> 296\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_grad()\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg\n",
      "File \u001b[0;32m~/miniconda3/envs/PT_GPU/lib/python3.12/site-packages/scipy/optimize/_differentiable_functions.py:262\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_fun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated:\n\u001b[0;32m--> 262\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/PT_GPU/lib/python3.12/site-packages/scipy/optimize/_differentiable_functions.py:163\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_fun\u001b[0;34m()\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_fun\u001b[39m():\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m \u001b[43mfun_wrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/PT_GPU/lib/python3.12/site-packages/scipy/optimize/_differentiable_functions.py:145\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnfev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[0;32m--> 145\u001b[0m fx \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(fx):\n",
      "File \u001b[0;32m~/miniconda3/envs/PT_GPU/lib/python3.12/site-packages/statsmodels/base/model.py:534\u001b[0m, in \u001b[0;36mLikelihoodModel.fit.<locals>.f\u001b[0;34m(params, *args)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mf\u001b[39m(params, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m--> 534\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloglike\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m nobs\n",
      "File \u001b[0;32m~/miniconda3/envs/PT_GPU/lib/python3.12/site-packages/statsmodels/tsa/statespace/mlemodel.py:940\u001b[0m, in \u001b[0;36mMLEModel.loglike\u001b[0;34m(self, params, *args, **kwargs)\u001b[0m\n\u001b[1;32m    937\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m complex_step:\n\u001b[1;32m    938\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minversion_method\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m INVERT_UNIVARIATE \u001b[38;5;241m|\u001b[39m SOLVE_LU\n\u001b[0;32m--> 940\u001b[0m loglike \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloglike\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomplex_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomplex_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    942\u001b[0m \u001b[38;5;66;03m# Koopman, Shephard, and Doornik recommend maximizing the average\u001b[39;00m\n\u001b[1;32m    943\u001b[0m \u001b[38;5;66;03m# likelihood to avoid scale issues, but the averaging is done\u001b[39;00m\n\u001b[1;32m    944\u001b[0m \u001b[38;5;66;03m# automatically in the base model `fit` method\u001b[39;00m\n\u001b[1;32m    945\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loglike\n",
      "File \u001b[0;32m~/miniconda3/envs/PT_GPU/lib/python3.12/site-packages/statsmodels/tsa/statespace/kalman_filter.py:1001\u001b[0m, in \u001b[0;36mKalmanFilter.loglike\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    986\u001b[0m \u001b[38;5;124;03mCalculate the loglikelihood associated with the statespace model.\u001b[39;00m\n\u001b[1;32m    987\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    997\u001b[0m \u001b[38;5;124;03m    The joint loglikelihood.\u001b[39;00m\n\u001b[1;32m    998\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    999\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconserve_memory\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1000\u001b[0m                   MEMORY_CONSERVE \u001b[38;5;241m^\u001b[39m MEMORY_NO_LIKELIHOOD)\n\u001b[0;32m-> 1001\u001b[0m kfilter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_filter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1002\u001b[0m loglikelihood_burn \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloglikelihood_burn\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1003\u001b[0m                                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloglikelihood_burn)\n\u001b[1;32m   1004\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconserve_memory\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m&\u001b[39m MEMORY_NO_LIKELIHOOD):\n",
      "File \u001b[0;32m~/miniconda3/envs/PT_GPU/lib/python3.12/site-packages/statsmodels/tsa/statespace/kalman_filter.py:924\u001b[0m, in \u001b[0;36mKalmanFilter._filter\u001b[0;34m(self, filter_method, inversion_method, stability_method, conserve_memory, filter_timing, tolerance, loglikelihood_burn, complex_step)\u001b[0m\n\u001b[1;32m    921\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize_state(prefix\u001b[38;5;241m=\u001b[39mprefix, complex_step\u001b[38;5;241m=\u001b[39mcomplex_step)\n\u001b[1;32m    923\u001b[0m \u001b[38;5;66;03m# Run the filter\u001b[39;00m\n\u001b[0;32m--> 924\u001b[0m \u001b[43mkfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m kfilter\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, X, y, sequence_length=24):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.FloatTensor(y)\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X) - self.sequence_length + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.X[idx:idx + self.sequence_length], \n",
    "                self.y[idx + self.sequence_length - 1])\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=128, num_layers=2, sequence_length=24):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.sequence_length = sequence_length\n",
    "        \n",
    "        self.batch_norm_input = nn.BatchNorm1d(sequence_length)\n",
    "        \n",
    "        # Simplified LSTM architecture\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, \n",
    "                           batch_first=True, dropout=0.2)\n",
    "        \n",
    "        # Simplified dense layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm_input(x)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        return self.fc(lstm_out[:, -1, :])\n",
    "\n",
    "class HybridAQIPredictor:\n",
    "    def __init__(self, sequence_length=24, hidden_size=128, num_layers=2):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.scaler_X = StandardScaler()\n",
    "        self.scaler_y = StandardScaler()\n",
    "        self.sarimax_model = None\n",
    "        self.lstm_model = None\n",
    "        \n",
    "    def prepare_data(self, data, state):\n",
    "        state_data = data[data['state'] == state].copy()\n",
    "        state_data = state_data.sort_values('timestamp')\n",
    "        \n",
    "        # Enhanced time-based features\n",
    "        timestamp = pd.to_datetime(state_data['timestamp'])\n",
    "        state_data['hour'] = timestamp.dt.hour\n",
    "        state_data['day_of_week'] = timestamp.dt.dayofweek\n",
    "        state_data['month'] = timestamp.dt.month\n",
    "        state_data['day_of_year'] = timestamp.dt.dayofyear\n",
    "        \n",
    "        feature_cols = ['co', 'no', 'no2', 'o3', 'so2', 'pm2_5', 'pm10', 'nh3',\n",
    "                       'temperature_2m', 'relative_humidity_2m', 'rain',\n",
    "                       'wind_speed_10m', 'wind_direction_10m',\n",
    "                       'soil_temperature_0_to_7cm', 'soil_moisture_0_to_7cm',\n",
    "                       'hour', 'day_of_week', 'month', 'day_of_year']\n",
    "        \n",
    "        X = state_data[feature_cols].values\n",
    "        y = state_data['aqi'].values\n",
    "        \n",
    "        # Split data\n",
    "        train_size = int(len(state_data) * 0.8)\n",
    "        test_size = len(state_data) - train_size\n",
    "        \n",
    "        X_train = X[:train_size]\n",
    "        X_test = X[train_size:]\n",
    "        y_train = y[:train_size]\n",
    "        y_test = y[train_size:]\n",
    "        \n",
    "        # Scale data\n",
    "        X_train_scaled = self.scaler_X.fit_transform(X_train)\n",
    "        X_test_scaled = self.scaler_X.transform(X_test)\n",
    "        \n",
    "        y_train_scaled = self.scaler_y.fit_transform(y_train.reshape(-1, 1))\n",
    "        y_test_scaled = self.scaler_y.transform(y_test.reshape(-1, 1))\n",
    "        \n",
    "        return (X_train_scaled, X_test_scaled,\n",
    "                y_train_scaled, y_test_scaled,\n",
    "                y_train, y_test)\n",
    "    \n",
    "    def train_sarimax(self, y_train, exog_train=None):\n",
    "        # Simplified SARIMAX for hourly data\n",
    "        self.sarimax_model = SARIMAX(y_train,\n",
    "                                   exog=exog_train,\n",
    "                                   order=(1, 1, 1),\n",
    "                                   seasonal_order=(1, 1, 1, 24))\n",
    "        self.sarimax_model = self.sarimax_model.fit(disp=False)\n",
    "        return self.sarimax_model.predict()\n",
    "    \n",
    "    def train_lstm(self, X_train, residuals, batch_size=32, epochs=50):\n",
    "        train_dataset = TimeSeriesDataset(X_train, residuals, self.sequence_length)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        input_size = X_train.shape[1]\n",
    "        self.lstm_model = LSTMModel(input_size, \n",
    "                                  self.hidden_size,\n",
    "                                  self.num_layers,\n",
    "                                  self.sequence_length).to(self.device)\n",
    "        \n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(self.lstm_model.parameters(), lr=0.001)\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            self.lstm_model.train()\n",
    "            total_loss = 0\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                X_batch = X_batch.to(self.device)\n",
    "                y_batch = y_batch.to(self.device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.lstm_model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            avg_loss = total_loss / len(train_loader)\n",
    "            scheduler.step(avg_loss)\n",
    "            \n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}')\n",
    "\n",
    "    def predict(self, X, exog=None):\n",
    "        if exog is not None:\n",
    "            sarimax_pred = self.sarimax_model.predict(exog=exog)\n",
    "        else:\n",
    "            sarimax_pred = self.sarimax_model.forecast(len(X))\n",
    "        \n",
    "        self.lstm_model.eval()\n",
    "        with torch.no_grad():\n",
    "            sequences = []\n",
    "            for i in range(len(X) - self.sequence_length + 1):\n",
    "                sequence = X[i:i + self.sequence_length]\n",
    "                sequences.append(sequence)\n",
    "            \n",
    "            if sequences:\n",
    "                sequences = torch.FloatTensor(np.array(sequences)).to(self.device)\n",
    "                lstm_preds = self.lstm_model(sequences).cpu().numpy()\n",
    "                \n",
    "                # Inverse transform predictions\n",
    "                lstm_preds = self.scaler_y.inverse_transform(lstm_preds)\n",
    "                sarimax_part = sarimax_pred[self.sequence_length-1:]\n",
    "                \n",
    "                # Ensure lengths match\n",
    "                min_len = min(len(sarimax_part), len(lstm_preds))\n",
    "                final_preds = sarimax_part[:min_len] + lstm_preds[:min_len].flatten()\n",
    "                \n",
    "                return final_preds\n",
    "            return np.array([])\n",
    "\n",
    "    def train_and_evaluate(self, data):\n",
    "        results = {}\n",
    "        states = data['state'].unique()\n",
    "        \n",
    "        for state in states:\n",
    "            print(f\"\\nTraining model for {state}\")\n",
    "            \n",
    "            (X_train_scaled, X_test_scaled,\n",
    "             y_train_scaled, y_test_scaled,\n",
    "             y_train, y_test) = self.prepare_data(data, state)\n",
    "            \n",
    "            # SARIMAX training\n",
    "            sarimax_preds_train = self.train_sarimax(y_train)\n",
    "            residuals = y_train - sarimax_preds_train\n",
    "            residuals_scaled = self.scaler_y.transform(residuals.reshape(-1, 1))\n",
    "            \n",
    "            # LSTM training on residuals\n",
    "            self.train_lstm(X_train_scaled, residuals_scaled)\n",
    "            \n",
    "            # Generate predictions\n",
    "            y_pred = self.predict(X_test_scaled)\n",
    "            \n",
    "            # Adjust test data length to match predictions\n",
    "            y_test_adj = y_test[self.sequence_length-1:self.sequence_length-1+len(y_pred)]\n",
    "            \n",
    "            if len(y_pred) > 0:\n",
    "                metrics = {\n",
    "                    'mse': mean_squared_error(y_test_adj, y_pred),\n",
    "                    'mae': mean_absolute_error(y_test_adj, y_pred),\n",
    "                    'r2': r2_score(y_test_adj, y_pred)\n",
    "                }\n",
    "                \n",
    "                results[state] = metrics\n",
    "                print(f\"MSE: {metrics['mse']:.4f}\")\n",
    "                print(f\"MAE: {metrics['mae']:.4f}\")\n",
    "                print(f\"R2 Score: {metrics['r2']:.4f}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def save_model(self, path):\n",
    "        model_data = {\n",
    "            'sarimax_model': self.sarimax_model,\n",
    "            'lstm_state_dict': self.lstm_model.state_dict(),\n",
    "            'scaler_X': self.scaler_X,\n",
    "            'scaler_y': self.scaler_y,\n",
    "            'hidden_size': self.hidden_size,\n",
    "            'num_layers': self.num_layers,\n",
    "            'sequence_length': self.sequence_length\n",
    "        }\n",
    "        torch.save(model_data, path)\n",
    "    \n",
    "    def load_model(self, path, input_size):\n",
    "        model_data = torch.load(path)\n",
    "        self.sarimax_model = model_data['sarimax_model']\n",
    "        self.scaler_X = model_data['scaler_X']\n",
    "        self.scaler_y = model_data['scaler_y']\n",
    "        self.hidden_size = model_data['hidden_size']\n",
    "        self.num_layers = model_data['num_layers']\n",
    "        self.sequence_length = model_data['sequence_length']\n",
    "        \n",
    "        self.lstm_model = LSTMModel(input_size, \n",
    "                                  self.hidden_size,\n",
    "                                  self.num_layers,\n",
    "                                  self.sequence_length).to(self.device)\n",
    "        self.lstm_model.load_state_dict(model_data['lstm_state_dict'])\n",
    "\n",
    "# Usage example\n",
    "sequence_length = 24\n",
    "model = HybridAQIPredictor(sequence_length=sequence_length)\n",
    "results = model.train_and_evaluate(data)\n",
    "\n",
    "# Save the model\n",
    "model.save_model('hybrid_aqi_model.pth')\n",
    "\n",
    "# Load the model\n",
    "input_size = 19  # Number of features\n",
    "new_model = HybridAQIPredictor(sequence_length=sequence_length)\n",
    "new_model.load_model('hybrid_aqi_model.pth', input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Forest Results:\n",
      "Train R2: 0.8829\n",
      "Test R2: 0.0230\n",
      "Train MSE: 30.0749\n",
      "Test MSE: 1788.5972\n",
      "Train MAE: 0.2009\n",
      "Test MAE: 2.2868\n",
      "R2 Gap (Train - Test): 0.8599\n",
      "\n",
      "XGBoost Results:\n",
      "Train R2: 0.9779\n",
      "Test R2: -0.3041\n",
      "Train MSE: 5.6809\n",
      "Test MSE: 2387.5138\n",
      "Train MAE: 0.2629\n",
      "Test MAE: 3.9195\n",
      "R2 Gap (Train - Test): 1.2820\n",
      "\n",
      "Gradient Boosting Results:\n",
      "Train R2: 0.9994\n",
      "Test R2: 0.0234\n",
      "Train MSE: 0.1546\n",
      "Test MSE: 1787.9062\n",
      "Train MAE: 0.1702\n",
      "Test MAE: 2.0999\n",
      "R2 Gap (Train - Test): 0.9760\n",
      "\n",
      "Lasso Results:\n",
      "Train R2: 0.9997\n",
      "Test R2: 0.8220\n",
      "Train MSE: 0.0703\n",
      "Test MSE: 325.8685\n",
      "Train MAE: 0.1860\n",
      "Test MAE: 0.7842\n",
      "R2 Gap (Train - Test): 0.1777\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LassoCV, RidgeCV\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "def create_time_features(df):\n",
    "    \"\"\"Create time-based features from timestamp\"\"\"\n",
    "    df = df.copy()\n",
    "    df['hour'] = df['timestamp'].dt.hour\n",
    "    df['day_of_week'] = df['timestamp'].dt.dayofweek\n",
    "    df['month'] = df['timestamp'].dt.month\n",
    "    df['day_of_year'] = df['timestamp'].dt.dayofyear\n",
    "    df['is_weekend'] = df['timestamp'].dt.dayofweek.isin([5, 6]).astype(int)\n",
    "    \n",
    "    # Create cyclical features for time variables\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['hour']/24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['hour']/24)\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['month']/12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['month']/12)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_lag_features(df, columns, lags=[1, 2, 3]):\n",
    "    \"\"\"Create lag features for specified columns\"\"\"\n",
    "    df = df.copy()\n",
    "    for col in columns:\n",
    "        for lag in lags:\n",
    "            df[f'{col}_lag_{lag}'] = df[col].shift(lag)\n",
    "    return df\n",
    "\n",
    "def create_rolling_features(df, columns, windows=[3, 6, 12]):\n",
    "    \"\"\"Create rolling mean and std features for specified columns\"\"\"\n",
    "    df = df.copy()\n",
    "    for col in columns:\n",
    "        for window in windows:\n",
    "            df[f'{col}_rolling_mean_{window}'] = df[col].rolling(window=window).mean()\n",
    "            df[f'{col}_rolling_std_{window}'] = df[col].rolling(window=window).std()\n",
    "    return df\n",
    "\n",
    "# Prepare the data\n",
    "alabama_data = data[data['state'] == 'Alabama'].copy()\n",
    "alabama_data['timestamp'] = pd.to_datetime(alabama_data['timestamp'])\n",
    "\n",
    "# Feature engineering\n",
    "alabama_data = create_time_features(alabama_data)\n",
    "\n",
    "# Create lag and rolling features for important columns\n",
    "important_columns = ['co', 'no2', 'o3', 'pm2_5', 'pm10', 'temperature_2m', 'wind_speed_10m']\n",
    "alabama_data = create_lag_features(alabama_data, important_columns)\n",
    "alabama_data = create_rolling_features(alabama_data, important_columns)\n",
    "\n",
    "# Drop rows with NaN values created by lag/rolling features\n",
    "alabama_data = alabama_data.dropna()\n",
    "\n",
    "# Prepare features and target\n",
    "features_to_drop = ['timestamp', 'state', 'aqi']\n",
    "X = alabama_data.drop(features_to_drop, axis=1)\n",
    "y = alabama_data['aqi']\n",
    "\n",
    "# Create time-based train-test split (last 20% of data for testing)\n",
    "split_idx = int(len(X) * 0.8)\n",
    "X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initialize models with expanded parameters\n",
    "models = {\n",
    "    'Random Forest': RandomForestRegressor(\n",
    "        n_estimators=500,\n",
    "        max_depth=20,\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=1,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    'XGBoost': XGBRegressor(\n",
    "        n_estimators=500,\n",
    "        max_depth=7,\n",
    "        learning_rate=0.01,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(\n",
    "        n_estimators=500,\n",
    "        max_depth=7,\n",
    "        learning_rate=0.01,\n",
    "        subsample=0.8,\n",
    "        random_state=42\n",
    "    ),\n",
    "    'Lasso': LassoCV(\n",
    "        cv=5,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    # Train model\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    train_pred = model.predict(X_train_scaled)\n",
    "    test_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    results[name] = {\n",
    "        'train_r2': r2_score(y_train, train_pred),\n",
    "        'test_r2': r2_score(y_test, test_pred),\n",
    "        'train_mse': mean_squared_error(y_train, train_pred),\n",
    "        'test_mse': mean_squared_error(y_test, test_pred),\n",
    "        'train_mae': mean_absolute_error(y_train, train_pred),\n",
    "        'test_mae': mean_absolute_error(y_test, test_pred)\n",
    "    }\n",
    "\n",
    "# Print results\n",
    "for name, metrics in results.items():\n",
    "    print(f\"\\n{name} Results:\")\n",
    "    print(f\"Train R2: {metrics['train_r2']:.4f}\")\n",
    "    print(f\"Test R2: {metrics['test_r2']:.4f}\")\n",
    "    print(f\"Train MSE: {metrics['train_mse']:.4f}\")\n",
    "    print(f\"Test MSE: {metrics['test_mse']:.4f}\")\n",
    "    print(f\"Train MAE: {metrics['train_mae']:.4f}\")\n",
    "    print(f\"Test MAE: {metrics['test_mae']:.4f}\")\n",
    "    print(f\"R2 Gap (Train - Test): {metrics['train_r2'] - metrics['test_r2']:.4f}\")\n",
    "\n",
    "# Get feature importance from the best performing model (assuming it's tree-based)\n",
    "best_model_name = max(results.items(), key=lambda x: x[1]['test_r2'])[0]\n",
    "if best_model_name in ['Random Forest', 'XGBoost', 'Gradient Boosting']:\n",
    "    best_model = models[best_model_name]\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': best_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(f\"\\nTop 10 Most Important Features ({best_model_name}):\")\n",
    "    print(feature_importance.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training RandomForest...\n",
      "\n",
      "Training XGBoost...\n",
      "\n",
      "Training Lasso...\n",
      "\n",
      "Best ML Model: Lasso\n",
      "Best R2 Score: 0.9990\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.linear_model import LassoCV\n",
    "import xgboost as xgb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Filter data for Alabama\n",
    "alabama_data = data[data['state'] == 'Alabama'].copy()\n",
    "\n",
    "# Prepare features and target\n",
    "feature_cols = ['co', 'no', 'no2', 'o3', 'so2', 'pm2_5', 'pm10', 'nh3', \n",
    "                'temperature_2m', 'relative_humidity_2m', 'rain', \n",
    "                'wind_speed_10m', 'wind_direction_10m', \n",
    "                'soil_temperature_0_to_7cm', 'soil_moisture_0_to_7cm']\n",
    "\n",
    "X = alabama_data[feature_cols]\n",
    "y = alabama_data['aqi']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define ML models and their parameter grids\n",
    "models = {\n",
    "    'RandomForest': {\n",
    "        'model': RandomForestRegressor(random_state=42),\n",
    "        'params': {\n",
    "            'n_estimators': [100, 200],\n",
    "            'max_depth': [10, 20, None],\n",
    "            'min_samples_split': [2, 5]\n",
    "        }\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'model': xgb.XGBRegressor(random_state=42),\n",
    "        'params': {\n",
    "            'n_estimators': [100, 200],\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'learning_rate': [0.01, 0.1]\n",
    "        }\n",
    "    },\n",
    "    'Lasso': {\n",
    "        'model': LassoCV(random_state=42),\n",
    "        'params': {\n",
    "            'eps': [1e-3, 1e-4],\n",
    "            'n_alphas': [100, 200],\n",
    "            'max_iter': [1000]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Train and evaluate ML models\n",
    "best_model = None\n",
    "best_score = -float('inf')\n",
    "results = {}\n",
    "\n",
    "for name, model_info in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    grid_search = GridSearchCV(\n",
    "        model_info['model'],\n",
    "        model_info['params'],\n",
    "        cv=5,\n",
    "        scoring='r2',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    grid_search.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Get best model and score\n",
    "    y_pred = grid_search.predict(X_test_scaled)\n",
    "    score = r2_score(y_test, y_pred)\n",
    "    results[name] = {\n",
    "        'model': grid_search.best_estimator_,\n",
    "        'params': grid_search.best_params_,\n",
    "        'score': score\n",
    "    }\n",
    "    \n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_model = grid_search.best_estimator_\n",
    "        best_name = name\n",
    "\n",
    "print(f\"\\nBest ML Model: {best_name}\")\n",
    "print(f\"Best R2 Score: {best_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/50], Train Loss: 0.0647, Val Loss: 0.0413\n",
      "Epoch [20/50], Train Loss: 0.0433, Val Loss: 0.0492\n",
      "Epoch [30/50], Train Loss: 0.0362, Val Loss: 0.0483\n",
      "Epoch [40/50], Train Loss: 0.0295, Val Loss: 0.0507\n",
      "Epoch [50/50], Train Loss: 0.0173, Val Loss: 0.0642\n",
      "\n",
      "Final Hybrid Model R2 Score: 0.9983\n"
     ]
    }
   ],
   "source": [
    "# Generate residuals using best ML model\n",
    "y_pred_train = best_model.predict(X_train_scaled)\n",
    "y_pred_test = best_model.predict(X_test_scaled)\n",
    "residuals_train = y_train - y_pred_train\n",
    "residuals_test = y_test - y_pred_test\n",
    "\n",
    "# LSTM Dataset\n",
    "class ResidualDataset(Dataset):\n",
    "    def __init__(self, features, residuals, sequence_length=24):\n",
    "        self.features = torch.FloatTensor(features)\n",
    "        self.residuals = torch.FloatTensor(residuals.values.reshape(-1, 1))\n",
    "        self.sequence_length = sequence_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.features) - self.sequence_length\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        X = self.features[idx:idx + self.sequence_length]\n",
    "        y = self.residuals[idx + self.sequence_length]\n",
    "        return X, y\n",
    "\n",
    "# LSTM Model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=64, num_layers=2):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        last_hidden = lstm_out[:, -1, :]\n",
    "        out = self.fc(last_hidden)\n",
    "        return out\n",
    "\n",
    "# Train LSTM\n",
    "def train_lstm(train_loader, val_loader, model, epochs=50):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                val_loss += criterion(outputs, y_batch).item()\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_lstm_model.pth')\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss/len(train_loader):.4f}, Val Loss: {val_loss/len(val_loader):.4f}')\n",
    "\n",
    "# Create LSTM datasets\n",
    "sequence_length = 24  # 24 hours\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = ResidualDataset(X_train_scaled, residuals_train, sequence_length)\n",
    "test_dataset = ResidualDataset(X_test_scaled, residuals_test, sequence_length)\n",
    "\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Initialize and train LSTM\n",
    "lstm_model = LSTMModel(input_size=len(feature_cols))\n",
    "train_lstm(train_loader, val_loader, lstm_model)\n",
    "\n",
    "# Function to make hybrid predictions\n",
    "def make_hybrid_predictions(X, ml_model, lstm_model, sequence_length=24):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # ML predictions\n",
    "    ml_pred = ml_model.predict(X)\n",
    "    \n",
    "    # Prepare data for LSTM\n",
    "    dataset = ResidualDataset(X, pd.Series(np.zeros(len(X))), sequence_length)\n",
    "    loader = DataLoader(dataset, batch_size=32)\n",
    "    \n",
    "    # LSTM predictions\n",
    "    lstm_model.eval()\n",
    "    lstm_preds = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, _ in loader:\n",
    "            # Move input batch to the same device as the model\n",
    "            X_batch = X_batch.to(device)\n",
    "            outputs = lstm_model(X_batch)\n",
    "            lstm_preds.extend(outputs.cpu().numpy())\n",
    "    \n",
    "    # Combine predictions\n",
    "    final_predictions = ml_pred[sequence_length:] + np.array(lstm_preds).flatten()\n",
    "    return final_predictions\n",
    "\n",
    "# Make final predictions\n",
    "final_predictions = make_hybrid_predictions(X_test_scaled, best_model, lstm_model)\n",
    "final_r2 = r2_score(y_test[24:], final_predictions)\n",
    "print(f\"\\nFinal Hybrid Model R2 Score: {final_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PT_GPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
